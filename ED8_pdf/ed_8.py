# -*- coding: utf-8 -*-
"""ED_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OzvFHV-jiC5CCjSKzPDmp9BWM4AXQ4qg

ZAD 1
"""

import tensorflow as tf

x = tf.Variable(4.0)
y = tf.Variable(3.0)

with tf.GradientTape() as tape:
    f = (x**3)+(y**2)                        
    df_dx,df_dy = tape.gradient(f,(x,y))

print(df_dx.numpy())
print(df_dy.numpy())

"""ZAD 2"""



x = tf.Variable(1.0)
y = tf.Variable(2.0)

with tf.GradientTape() as tape:
    f = 4*(x**3)+11*(y**2)+9*y*x+10                         
    df_dx,df_dy = tape.gradient(f,(x,y))

print(df_dx.numpy())
print(df_dy.numpy())

"""Zad 3"""

import matplotlib.pyplot as plt 
import numpy as np

number_of_points = 1000
x_point = []
y_point = []

a = -0.22
b = 0.78

for i in range(number_of_points):
    x = np.random.normal(0.0,0.5)
    y = (a*x+b)+np.random.normal(0.0,0.1)
    x_point.append(x)
    y_point.append(y)

plt.scatter(x_point,y_point,c='b')
plt.show()

real_x = np.array(x_point)
real_y = np.array(y_point)

def loss_fn(real_y, pred_y):
    return tf.reduce_mean((real_y - pred_y)**2)

import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

Loss = []
epochs = 1000
learning_rate = 0.01

for _ in range(epochs):
  with tf.GradientTape() as tape:
    pred_y = a * real_x + b
    loss = loss_fn(real_y, pred_y)
    Loss.append(loss.numpy())

  dloss_da, dloss_db = tape.gradient(loss,(a, b))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db

np.max(Loss),np.min(Loss)

print(a.numpy())
print(b.numpy())

plt.scatter(np.arange(epochs),Loss)
plt.show()

max = np.max(x_point)
min = np.min(x_point)

X = np.linspace(min, max, num=10)
plt.plot(X,a.numpy()*X+b.numpy(),c='r')
plt.scatter(x_point,y_point,c="b")
plt.show()

def subset_dataset(x_dataset, y_dataset, subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr) 
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    return x_train,y_train

a = tf.Variable(random.random())
b = tf.Variable(random.random())

Loss = []
epochs = 1000
learning_rate = 0.2
batch_size = 50

for i in range(epochs):
  real_x_batch,real_y_batch = subset_dataset(real_x,real_y,batch_size)
  with tf.GradientTape() as tape:
    pred_y = a * real_x_batch + b
    loss = loss_fn(real_y_batch, pred_y)
    Loss.append(loss.numpy())

  dloss_da, dloss_db = tape.gradient(loss,(a, b))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db

np.max(Loss),np.min(Loss)

print(a.numpy())
print(b.numpy())

plt.plot(Loss)
plt.show()



"""ZAD 4"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
df = pd.read_csv('Boston.csv')
print(df)

df.head()

rm=df.iloc[:,6]

rm

medv=df.iloc[:,14]

medv

plt.figure(figsize=(20, 5))

features = ['rm']
target = df['medv']

for i, col in enumerate(features):
    plt.subplot(1, len(features) , i+1)
    x = df[col]
    y = target
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('medv')

real_rm = np.array(rm)
real_medv = np.array(medv)

real_rm

import random
a = tf.Variable(random.random())
b = tf.Variable(random.random())

def subset_dataset(x_dataset, y_dataset, subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr) 
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    return x_train,y_train

def loss_fn(real_y, pred_y):
    return tf.reduce_mean((real_y - pred_y)**2)



Loss = []
epochs = 1000
learning_rate = 0.01
batch_size = 50
a = tf.Variable(random.random())
b = tf.Variable(random.random())
for _ in range(epochs):
  real_rm_batch,real_medv_batch = subset_dataset(real_rm,real_medv,batch_size)
  with tf.GradientTape() as tape:
    pred_medv = a * real_rm_batch + b
    loss = loss_fn(real_medv_batch, pred_medv)
    Loss.append(loss.numpy())

  dloss_da, dloss_db = tape.gradient(loss,(a, b))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db

Loss

np.max(Loss),np.min(Loss)

print(a.numpy())
print(b.numpy())

import tensorflow as tf
import keras 
from keras.layers import Dense
from keras.models import Sequential

model=Sequential()

model.add(Dense(units = 3, use_bias=True, input_dim=1, activation = "linear"))
model.add(Dense(units = 1, use_bias=True,  activation = "linear"))

opt = tf.keras.optimizers.Adam()

model.compile(loss='MSE',optimizer=opt)

model.summary()

epochs = 200
h = model.fit(real_rm,real_medv, verbose=1, epochs=epochs, batch_size=100, validation_split=0.3)

#Loss = h.history['loss']
#Loss

plt.scatter(np.arange(epochs),h.history['loss'])
plt.scatter(np.arange(epochs),h.history['val_loss'],c='r')
plt.show()

"""Zadanie 5"""

import tensorflow as tf
import matplotlib.pyplot as plt 
import numpy as np

import keras
from keras.models import Sequential
from keras.layers import Dense

"""Dwa gangi

Zbi√≥r danych:
"""

[0]*10+[1]*10

x_label1 = np.random.normal(3, 1, 1000)
y_label1 = np.random.normal(2, 1, 1000) 
x_label2 = np.random.normal(7, 1, 1000)
y_label2 = np.random.normal(6, 1, 1000)

xs = np.append(x_label1, x_label2)
ys = np.append(y_label1, y_label2) 
labels = np.asarray([0.]*len(x_label1)+[1.]*len(x_label2))
labels

plt.scatter(x_label1, y_label1, c='black', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='brown', marker='1', s=20)
plt.show()

x_label1

def loss_fn_grad(y, y_model):
 return tf.reduce_mean(-y*tf.math.log(y_model)-(1-y)*tf.math.log(1-y_model))

def subset_dataset_2(x_dataset, y_dataset,label,subset_size):
    arr = np.arange(len(x_dataset))
    np.random.shuffle(arr) 
    x_train = x_dataset[arr[0:subset_size]]
    y_train = y_dataset[arr[0:subset_size]]
    label_train = label[arr[0:subset_size]]
    return x_train,y_train,label_train

labels.shape

Loss = []
epochs = 1000
learning_rate = 0.01
batch_size = 50
a = tf.Variable(random.random())
b = tf.Variable(random.random())
c = tf.Variable(random.random())
for _ in range(epochs):
  xs_batch,ys_batch,labels_batch = subset_dataset_2(xs,ys,labels,batch_size)
  with tf.GradientTape() as tape:
    pred_l = tf.sigmoid(a * xs_batch + b * ys_batch + c)
    #print(label_batch.shape)
    loss = loss_fn_grad(labels_batch, pred_l)
    Loss.append(loss.numpy())

  dloss_da, dloss_db, dloss_dc = tape.gradient(loss,(a, b,c))

  a.assign_sub(learning_rate*dloss_da)  #a = a - alpha*dloss_da
  b.assign_sub(learning_rate*dloss_db)  #b = b - alpha*dloss_db
  c.assign_sub(learning_rate*dloss_dc)

np.max(Loss),np.min(Loss)

print(a.numpy())
print(b.numpy())

plt.plot(Loss)
plt.show()

x=3.0
y=2.0 
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=7.0
y=6.0 
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()

x=4.0
y=5.0 
plt.scatter(x_label1, y_label1, c='r', marker='x', s=20)
plt.scatter(x_label2, y_label2, c='g', marker='1', s=20)
plt.scatter(x,y,c='b', marker='s')
plt.show()